# -*- coding: utf-8 -*-
"""MACHINE LEARNING-BASED CLASSIFICATION FOR TITANIC SURVIVAL  PREDICTION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16z1I5ba2UWaD8gDDbw0ODBtfrzWSqNHW

### MACHINE LEARNING-BASED CLASSIFICATION FOR TITANIC SURVIVAL PREDICTION

### Step1: Importing the Required Dependences
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
from sklearn.neural_network import MLPClassifier

"""## Explaining the usage of the imported libraries in the project
i.   Pandas: Pandas will be used for data manipulation and Exploratory Data Analysis (EDA)

ii.  NUmpy:  The numpy library will be used for numerical computation.

iii. Matplotlib & Seaborn: These libraries used be used for data visualization.

iv.  train_test_split from sklearn.model_selection: used for data splitting into training and test sets

v.   Accuracy_score, precision_score, recall_score from sklearn.metrics: These metrics are used in evaluating the model performance.

## Step 2: Datasource

The data set for this task was obtained from : https://www.kaggle.com/datasets/yasserh/titanic-dataset

## Step 3: Loading the dataset obtained from kaggle
"""

df=pd.read_csv('/content/Titanic-Dataset.csv')

"""# Step4:Viewing The Dataset Of Machine Learning-Based Classification For Titanic Survival  Prediction

df

# Step4: Access Data With Pandas Method

# Get basic information about this dataset Using Pandas Method df.info()
"""

df.info()

"""# Checking for the total number of columns in a dataset using pandas method df.shape"""

df.shape

"""In The Titanic dataset, There are 891 Records(Rows) and 12 Columns(Features)

# Checking For The Columns Name in The Titanic Dataset Using df.columns Method In Pandas df.columns
"""

df.columns

"""# Using .loc function in Pandas is use for Selecting Row by Label"""

df.loc[5]

df.loc[5,'PassengerId']

df.loc[0:4,['Ticket','Age']]

"""# Using .iloc Funaction in Pandas:When Using Iloc You Specity The Index Number Of The Row And Columns To Retrieve Data"""

df.iloc[0:5,1:7]

"""# Using df.head() in pandas to check for The First Top Five Row In The Titanic Dataset"""

df.head()

"""# Using df.tail() in pandas to check for The Last Five Row In The Titanic Dataset"""

df.tail()

"""# Using df.describe().T in Pandas to Generate Summary Statistics For Numerical Columns in a Dataset"""

df.describe().T

"""## Step 5: Checking Missing Values Using Pandas Method df.isnull().sum()"""

# checking for missing values in each feature
df.isnull().sum()

"""## Describing our findings about the missing values by Features

After using the pandas isnull() with the sum() function, three variables namely: Age, Cabin and Embarked were identified with 177, 687 and 2 missing vlues respectively

# checking the the total number of missing values in the entire dataset

df.isnull().sum().sum()
"""

df.isnull().sum().sum()

"""From the value shown above, a total of 866 missing values are present in the dataset

# Step6: Handling Missing Values in the Dataset
"""

df['Age'] =df['Age'].fillna(value =df['Age'].mean())

df

"""# checking if the missing values in the Age feature as been handled
df['Age'].isnull().sum()
"""

df['Age'].isnull().sum()

"""# Filling the missing values in the Cabin Values using Backward fill method in pandas
df['Cabin'] = df['Cabin'].bfill()
"""

df['Cabin'] = df['Cabin'].bfill()

df

"""# checking if there is any missing values in the Cabin Values after using the Backward fill method in pandas"""

df['Cabin'].isnull().sum()

"""After filling the Cabin variable using the Backward fill method, we noticed that there is still 1 missing cell, and we are going to fill  using the forward fill method in pandas"""

df['Cabin'] = df['Cabin'].ffill()

df

"""checking if there is any missing values in cabin veraible"""

df['Cabin'].isnull().sum()

df

"""Filling the missing values in the Embarked Values using Backward fill method in pandas

"""

df['Embarked'] = df['Embarked'].bfill()

df

# checking for the total values in Embarked variable
df['Embarked'].isnull().sum()

"""# After filling all the missing values, let's verify if there are still missing values in the dataset"""

# verifying if there are still missing values in each features
df.isnull().sum()

# verifying if there are still missing values in the entire dataset
df.isnull().sum().sum()

"""# Step 7: Checking for Duplicate data entries  Of Machine Learning-Based Classification For Titanic Survival Prediction

"""

df.duplicated()

"""# Step 7: Checking for Total sum of Duplicate data entries Of Machine Learning-Based Classification For Titanic Survival Prediction"""

df.duplicated().sum()

"""After checking For Duplicate Data Entries Of Machine Learning-Based Classification For Titanic Survival Prediction  we Discover That There Was No Duplicate in dataset Of Titanic Survival Prediction

# Step 8:How To Filter Single Columns In The Categorical Data OF Machine Learning-Based Classification For Titanic Survival Prediction¶
1.Filter By A Single Category(example,Only Female)
"""

df[df['Sex']=='female']
print(df.head())

"""# # Step 9:How To Filter Multiple Columns In The Categorical Data OF Machine Learning-Based Classification For Titanic Survival Prediction¶
2.Filter By Mutiple Categorries(example,Embarked From 'S' Or 'C')
"""

df[df['Embarked'].isin(['S','C'])]

"""# Checking For Maximum Value,Value_Counts and Minimum value Within A Specific Columns in Pandas DataFrame Of Titanic Survival Predication"""

df['Sex'].value_counts()

df['Sex'].value_counts().max()

df['Sex'].value_counts().min()

"""#  Analyzing and Visualizing the Survived passenger based on Pclass"""

# Performing bivariate analysis on the Pclass and the survived variable

df.groupby('Survived')['Pclass'].value_counts()

# convert the Survived and Pclass from string data type to integer before performing the analysis
df['Survived'] =  df['Survived'].astype(int)
df['Pclass'] =  df['Pclass'].astype(str)

# setting a background color for the chart
plt.style.use('ggplot')
# plotting the countplot for the visualization
sns.countplot(x="Survived", data=df, hue="Pclass")
#Giving the plot a title
plt.xlabel("Survived")
plt.ylabel("frequency")
plt.title("Survivor Count by Passenger Class")
plt.legend(title="Pclass")
plt.show()

"""# Interpreting the Chart Above

This chart shows how many people lived or died based on their class. Most who died were in class 3, while class 1 had the most survivors. Fewer people from class 2 survived. It means richer passengers in class 1 had a better chance to survive than those in lower classes.

# Analyzing and Visualizing the Survived passenger based on Gender
"""

df.groupby('Survived')['Sex'].value_counts()

# visualizing the result of the analysis

# convert the Survived and Pclass from string data type to integer before performing the analysis
df['Survived'] =  df['Survived'].astype(int)
df['Sex'] =  df['Sex'].astype(str)

# setting a background color for the chart
plt.style.use('bmh')
# plotting the countplot for the visualization
sns.countplot(x="Survived", data=df, hue="Sex")
#Giving the plot a title
plt.xlabel("Survived")
plt.ylabel("frequency")
plt.title("Survivor Count by Gender")
plt.legend(title="Gender")
plt.show()

"""# Interpreting the Chart Above
This chart shows how many men and women lived or died. 468 men and 81 women died. 233 women and 109 men survived. More women survived than men, meaning women had a better chance of survival in the titanic story.

# Analysis and Visualizing the Age variable to uncover how many youger, youth and old persons boarded the ship
"""

# converting the Age variable into integer data type
df['Age']=df['Age'].astype(int)

df

# since the Age variable is a continuous variable, we will apply the pandas cut method to segment the Age into three category [younger, youth and Old]
df['Age_Category'] =pd.cut(df['Age'], bins=3, labels=['youger', 'youth', 'old'])

df

# grouping the age distribution

df.groupby('Survived')['Age_Category'].value_counts()

import warnings
warnings.filterwarnings("ignore")


plt.style.use('bmh')
sns.countplot(x="Age_Category", data=df)
#Giving the plot a title
plt.xlabel("Age Category")
plt.ylabel("frequency")
plt.title("Distribution of Age group")
plt.show()



"""# Interpreting Univarate Analysis on The Age_category Varaible
The chart above shows the highiest age group in the titanic dataset is the youth having above 500 frequency. This is followed by the youger persons with above 300 frequency and fewer older persons.

# Performing bivariate analysis on the Age Category and Survived variables
# Plot the countplot
"""

sns.countplot(x="Survived", data=df, hue="Age_Category")
# Set labels and title
plt.xlabel("Age Category")
plt.ylabel("Frequency")
plt.title("Distribution of Age Group by Survived")
# Show the plot
plt.show()

"""#  Interpreting the Chart Above
The chart shows that the youth category had the highest number of individuals, with over 500 passengers, of which 326 did not survive and 196 survived. The younger group followed, with over 300 individuals, where 189 did not survive and 130 survived. The old category had the fewest passengers, with 34 not surviving and only 16 surviving. Overall, in all age categories, more people did not survive than those who did, but the youth and younger groups had higher survival counts compared to older individuals. This suggests that older individuals had the lowest survival rate, possibly due to physical limitations or lower priority in rescue efforts.

# Step10: Data Cleaning

We will apply the python regular expressing to creating patterns and filter out all special characters such (/,. ) and text from the Name and Ticket Variable
"""

# importing the regular expression library
import re

# defining a pattern that filters out all special characters

def filter(text):
    return re.sub(r'[^a-zA-Z0-9\s]', '', text)

# applying the pattern to filter out all the special character in the Name Variable
df["Name"]= df["Name"].apply(filter)

df

# applying the pattern to filter out all the special character and Texts in the Ticket Variable
df["Ticket"]= df["Ticket"].apply(filter)

# Remove all-numeric character from'Ticket'
df['Ticket']=df['Ticket'].str.replace(r'\D','',regex=True)

df

"""# Removing Non-Numeric Characters on a Ticket Variable Using str.replace() Method in Pandas"""

df

"""#  Step 11: Data Encoding"""



"""# We will apply the LabelEncoder and Pandas Map method to Encode all string variables to numerical values"""

# importing LabelEncoder Technique
from sklearn.preprocessing import LabelEncoder

# instantiating the labelEncoder
encoder=LabelEncoder()

# encoding the gender variable
df['Sex']=encoder.fit_transform(df['Sex'])

df



# encoding the Cabin variable
df['Cabin']=encoder.fit_transform(df['Cabin'])

df

# encoding the Embarked and Age Category Variables using Pandas map method

# creating a dictionary for the map values in the  Embarked variable
map={'S':0, 'C':1, 'Q':2}
# creating a dictionary for the map values in the  Age_Category variable
data={'youger':0, 'youth':1, 'old':2}

# encoding the values using the dictionary values
df['Embarked']= df['Embarked'].map(map)

df['Age_Category']= df['Age_Category'].map(data)

df

"""# Droping  The Column'Name' Permanently Using Pandas Method drop()"""

df.drop(columns=['Name'],inplace=True)

df

"""# checking for the correlation between variables in the dataset
# using the heatmap to viualize the correlation between variables in the datase

    
"""

df['Fare']=df['Fare'].astype(int)



# Convert Ticket Column from float to interger
df['Ticket_numeric'] = df['Ticket'].str.extract('(\d+)', expand=False)
df['Ticket']=df['Ticket_numeric'].fillna(0).astype(int)

df

# apply heatmap to check for feature correlations

correlation_matrix = df.corr()
# defining the size of the plot region
plt.figure(figsize=(10, 6))
# plotting the heatmap
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Feature Correlation Heatmap")
plt.show()

"""# Applying Chi-square for Feature Selection"""

# splitting the dataset into x and y variables

x = df.drop(columns=['Survived'])  # Independent variables
y = df['Survived']  # Target variable
x

x

y

# Performing feature selection using chi-square

# importing the selectBest and Chi2 libraries
from sklearn.feature_selection import SelectKBest, chi2



# Applying Chi-Square feature selection
chi_selector = SelectKBest(chi2, k=5)  # Select top 5 features
chi_selector.fit(x.fillna(0), y.fillna(0))

# Getting selected features
selected_features = x.columns[chi_selector.get_support()]
print("Top Features using Chi-Square:", list(selected_features))

"""# Step 12: Data Scaling using Standard Scaler"""

# since 'Pclass', 'Sex', 'Age', 'Fare', 'Embarked' are the important features, we will use apply this as the inputs
x = df[['Pclass', 'Sex', 'Age', 'Fare', 'Embarked']]

df

#Split data into training and test sets
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)

# instantiating the standardScaler
scaler = StandardScaler()

# stardardizing the input feature so the values could be between the range of 0 and 1
x_scale=scaler.fit_transform(x)

print(x_scale)

"""#  Step13: Data Spliting"""

# applying the train_test_split() to divide the dataset into training and test data
xtrain, xtest, ytrain, ytest=train_test_split(x_scale, y, train_size=0.8, test_size=0.2, random_state=42)

"""# Step 14: Model Training

# Training Logistic Regression Model
"""

# instantiating the model
model1 = LogisticRegression()

# training the model
model1.fit(xtrain, ytrain)

# predicting the target using the input features
y_predict= model1.predict(xtrain)
y_predict

"""#  Evaluating the Logistic Regression Model Training Accuracy"""

model1_training_accuracy = accuracy_score(y_predict, ytrain)

print('The training accuarcy for the logistics Regression model is: ' , round(model1_training_accuracy,2))

"""# Evaluating the Logistic Regression Model Test Accuracy

"""

test_predict= model1.predict(xtest)
test_predict

model1_test_accuracy = accuracy_score(test_predict, ytest)

print('The test accuarcy for the logistics Regression model is: ' , round(model1_test_accuracy, 2) )

"""EVALUATING THE LOGISTIC REGRESSION MODEL USING F1 score, Recall, Precision and Classification Report"""

# Convert ytest to numeric type to ensure consistency with y_pred
ytest = pd.to_numeric(ytest, errors='coerce')
y_pred = model1.predict(xtest)

# Calculate and print the metrics
print("Classification Report:\n", classification_report(ytest, y_pred))
print("Precision:", precision_score(ytest, y_pred, average='weighted'))
print("Recall:", recall_score(ytest, y_pred, average='weighted'))
print("F1 Score:", f1_score(ytest, y_pred, average='weighted'))

"""# Training SVM Model"""

# instantiating the model
model2 = svm.SVC(kernel='linear')

# training the svm model
model2.fit(xtrain, ytrain)

# predicting the target using the input features
y_predict=model2.predict(xtrain)
y_predict

"""#  Evaluating the SVM Model Training Accuracy"""

model2_training_accuracy = accuracy_score(y_predict, ytrain)

print('The training accuarcy for the SVM model is: ' , round(model2_training_accuracy, 2))

"""# Evaluating the SVM Model Test Accuracy"""

model2_test_predict= model2.predict(xtest)
model2_test_predict

model2_test_accuracy = accuracy_score(test_predict, ytest)
print('The test accuarcy for the SVM model is: ' , round(model2_test_accuracy, 2))

# Convert ytest to numeric type to ensure consistency with y_pred
ytest = pd.to_numeric(ytest, errors='coerce')
y_pred = model2.predict(xtest)

# Calculate and print the metrics
print("Classification Report:\n", classification_report(ytest, y_pred))
print("Precision:", precision_score(ytest, y_pred, average='weighted'))
print("Recall:", recall_score(ytest, y_pred, average='weighted'))
print("F1 Score:", f1_score(ytest, y_pred, average='weighted'))

"""#  Training Decision Tree Classifier Model"""

# importing the machine learning algorithm
from sklearn.tree import DecisionTreeClassifier

model3= DecisionTreeClassifier()

model3.fit(xtrain, ytrain)

# predicting the target using the inputs features
y_predict= model3.predict(xtrain)
y_predict

"""#  Evaluating the Decision Tree Model Training Accuracy"""

# evaluating the training accuracy
model3_trainin_accuracy = accuracy_score(y_predict, ytrain)
print('Training accuracy for decision tree model is:, ', round(model3_trainin_accuracy, 2))

"""#  Evaluating the Decision Tree Model Test Accuracy"""

test_predict= model3.predict(xtest)
test_predict

# evaluating the training accuracy
model3_test_accuracy = accuracy_score(test_predict, ytest)
print('Test accuracy for decision tree model is:, ', round(model3_test_accuracy, 2))

"""# Step 15: Training The ANN Model"""

x = df.iloc[:,:-1]
y =df.iloc[:,-1]

# Spliting data into traing and testing sets(80%,20%)
xtrain, xtest, ytrain, ytest=train_test_split(x_scale, y, train_size=0.8, test_size=0.2, random_state=42)

# instantiating the standardScaler
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

"""# Train The ANN Model"""

ann = MLPClassifier(hidden_layer_sizes=(100,50),max_iter=500,activation='relu',solver='adam',random_state=42)

# Train The ANN Model
ann.fit(x_train,y_train)

"""# Make On ANN Model Predictions"""

y_pred = ann.predict(x_train)
ann_train_accuracy = accuracy_score(y_train,y_pred)
print('ANN Model Training Accuracy:',ann_train_accuracy)

"""# Evaluating the ANN Model"""

test_predict= ann.predict(x_test)
test_predict

ann_test_accuracy = accuracy_score(y_test,y_test)
print('ANN Model Test Accuracy:',ann_test_accuracy)

"""# The Four Model Training and Test Evaluation Comparison chart"""

# Storing the model, training and testing accuracies in list containers
models = ['Logistic Regression', 'SVM', 'Decision Tree', 'ANN']
train_accuracy = [round(model1_training_accuracy,2), round(model2_training_accuracy, 2), round(model3_trainin_accuracy, 2), round(ann_train_accuracy, 2) ]
test_accuracy = [ round(model1_test_accuracy, 2), round(model2_test_accuracy, 2), round(model3_test_accuracy, 2),round(ann_test_accuracy, 1) ]

# Creating a DataFrame for the model, training and test accuracies
df = pd.DataFrame({
    'Model': models,
    'Train Accuracy': train_accuracy,
    'Test Accuracy': test_accuracy,
})

# Ploting bar chart comparison
plt.figure(figsize=(10, 6))
df.set_index("Model").plot(kind='bar', figsize=(12, 6), colormap='viridis')
plt.title("Comparison of Model Performance Metrics")
plt.ylabel("Score")
plt.xlabel("Model")
plt.ylim(0, 1)
plt.legend(loc="best")
plt.xticks(rotation=0)
plt.grid(axis='y', linestyle="--", alpha=0.7)

# Showing the plot
plt.show()

